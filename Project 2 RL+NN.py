import random
import numpy as np
from collections import deque
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import matplotlib.pyplot as plt


period = 15
GAMMA = 0.5
LEARNING_RATE = 0.001
episods=1650
MEMORY_SIZE = 1000   # the trip steps are about 300-700
BATCH_SIZE = 100     # batch size can change  in 20 50 100

EXPLORATION_MAX = 0.1# epsilon decay form 0.1 because larger epsilon will make training slower, agent will break frequently and early



def Do_action(n):  # given action number ,return direction of action

    if n == 0:
        return 0
    elif n == 1:
        return 0.01
    elif n == 2:
        return -0.01
    elif n == 3:
        return 0.02
    else :
        return -0.02

class prob():
    def __init__(self):
        self.x=0
        self.y=0
        self.center=20
        self.z=15.6
        self.setforce=1/25
        self.signal=self.z/2
        self.Hysteresis=0.1
        self.setheight = 5

    def operate(self,signal):
        if self.signal<signal:
            if signal<0.5*(15-self.signal)+self.signal:
                x1=self.signal
                y1=self.z
                x2=0.5*(15-self.signal)+self.signal
                y2=(0.5*(30-self.z)+self.z)*(1-self.Hysteresis)
                a=(y2-y1)/(x2-x1)
                b=y2-(a*x2)
                self.z=a*signal+b
            else:
                x1 = 0.5*(15-self.signal)+self.signal
                y1 = (0.5*(30-self.z)+self.z)*(1-self.Hysteresis)
                x2 = 15
                y2 = 30
                a = (y2 - y1) / (x2 - x1)
                b = y2 - (a * x2)
                self.z = a * signal + b


        if self.signal>signal:
            if signal<0.5*(self.signal-0):
                x1=0.5*self.signal
                y1=0.5*self.z*(1+self.Hysteresis)
                x2=0
                y2=0
                a=(y2-y1)/(x2-x1)
                b=y2-(a*x2)
                self.z=a*signal+b
            else:
                x1 = self.signal
                y1 = self.z
                x2 = 0.5*self.signal
                y2 = 0.5*self.z*(1+self.Hysteresis)
                a = (y2 - y1) / (x2 - x1)
                b = y2 - (a * x2)
                self.z = a * signal + b
        self.signal=signal

def force_feedback(distance):
    return 1/(distance*distance)
def ifterminal(next_state):
    '''
    according to next state, the DQN can make the thresh value more accuracy, 1200 is uniformization value test from traditional RL file
    '''
    if next_state>250/1200 or next_state<23/1200:
        return True

    else:return False


map=np.array([13,14,15,16,17,18,19,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,3,4,5,6,5,4,3,5,6,7,8,9,8,7,6,5,4,3])
print(map)



class DQNSolver:
    #this model scretch from
    def __init__(self, observation_space=1, action_space=5):
        self.exploration_rate = EXPLORATION_MAX                 #initialize the model
        self.action_space = action_space
        self.memory = deque(maxlen=MEMORY_SIZE)
        self.model = Sequential()                               #use the seqential basic model,size is 1*24*4
        self.model.add(Dense(24, input_shape=(observation_space,), activation="relu"))
        self.model.add(Dense(24, activation="relu"))
        self.model.add(Dense(self.action_space, activation="linear"))
        self.model.compile(loss="mse", optimizer=Adam(lr=LEARNING_RATE))# mse effect is much better than mae

    def remember(self, state, action, reward, next_state, done):        #store information generated by one step
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):                                       #predict action under epsilon policy
        if np.random.rand() < self.exploration_rate:
            return random.randrange(self.action_space)
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def experience_replay(self):                          # put some sample into a batch randomly,each sample will be learned
        if len(self.memory) < BATCH_SIZE:
            return
        batch = random.sample(self.memory, BATCH_SIZE)
        for state, action, reward, state_next, terminal in batch:
            q_update = reward
            if not terminal:
                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0])) #according to Q-learning algorithhm, if it is terminal state q value set as -1
            q_values = self.model.predict(state)#  if  not terminal ,the DQN give the predict value(5 action value)
            q_values[0][action] = q_update      # get the target action,then updated it.

            self.model.fit(state, q_values, verbose=0)     # verbose=0 means dont shown the log




list_height=[]
observation_space = 1
action_space = 5
dqn_solver = DQNSolver(observation_space, action_space)
dqn_solver.model.load_weights('my_model_p20_15.h5')
print('has load')
dqn_solver.exploration_rate=0
run = 0
list_step=[0]
list_z_=[]
list_x_=[]
list_error_=[]
list_height_=[]
for episod in range(episods):
    print('new episod------------------------------------',episod,'-----------------------------------------')
    agent = prob()
    height = map[agent.x // period]
    run += 1

    state=round(force_feedback(agent.z-height),3)*1000/1200
    state = np.reshape(state, [1, observation_space])
    step = 0

    list_x=[]
    list_z=[]
    list_height=[]
    list_error=[]
    while True:
        height=map[agent.x//period]          #  record step height
        list_height.append(height)
        step += 1

        action = dqn_solver.act(state)       # dqn give out its predict action according to state
        output = agent.signal + Do_action(action) * 10  #add adjust value to current signal
        agent.operate(output)                                            #operate on new signal
        list_z .append(agent.z)                                          #record height of agent
        list_error.append(np.abs(agent.z-height-agent.setheight))        #record error
        state_next=round(force_feedback(agent.z-height),3)*1000/1200     #calculate current state
        reward=1/(1+(np.abs(agent.setforce-force_feedback(agent.z-height)*5)))
        #reward function is similar to traditional RL while muliply 5 is to accelerate learning
        terminal= ifterminal(state_next)

        reward = reward if not terminal else -1

        state_next = np.reshape(state_next, [1, observation_space])    #calculate state

        dqn_solver.remember(state, action, reward, state_next, terminal) #record 5 information in a unit into deque queue
        print('run',run,'step',step,'exploration_rate', dqn_solver.exploration_rate,'max setp',max(list_step))
        state = state_next                                              #update state


        if terminal:
            list_step.append(step)
            break
        agent.x += 1
        list_x.append((agent.x))
        dqn_solver.experience_replay()   #start learning.

        if agent.x ==len(map)*period:
            print('----------------------------------Got final--------------------------------------------')
            #if finish one trip, copy the temperate data to a chart table
            list_step.append(step)
            list_x_=np.copy(list_x)
            list_z_=np.copy(list_z)
            list_error_=np.copy(list_error)
            list_height_=np.copy(list_height)

            print('error per unit:',(np.sum(list_error_))/(period*len(map)))

            list_step.append(step)
            #save the trained model
            dqn_solver.model.save('my_model_p20_15.h5')
            print('has save')

            if  dqn_solver.exploration_rate==0:
                # when epsilon decay to zero start to show the chart
                plt.scatter(list_x_, list_height_, s=1)
                plt.scatter(list_x_, list_z_, s=1)
                plt.scatter(list_x_, list_error_, s=1)
                plt.show()

            dqn_solver.exploration_rate *= 0.1
            if dqn_solver.exploration_rate <=0.1:
                dqn_solver.exploration_rate*= 0.1
            if dqn_solver.exploration_rate <=0.0001:
                dqn_solver.exploration_rate= 0
                # when the epsilon decay form 0.1, every finished trip ,epsilon dacay 90%,when it smaller than 0.0001 set 0 and continue training.





            print('----------------------------------Got final--------------------------------------------')
            break




